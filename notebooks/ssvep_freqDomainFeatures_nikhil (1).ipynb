{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook has the code to append respective frequency domain features (FFT, avgPow, totalPow, weighted-avg-frequencies) to the corresponding time-series segments."
      ],
      "metadata": {
        "id": "vMNacFadJ8CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "# Assuming you have your data X and labels y defined\n",
        "# Make sure that your data is appropriately shaped, e.g., (num_samples, sequence_length, num_features)\n",
        "freq8 = pd.read_csv('/content/drive/MyDrive/LeBrain - Neureality 2024/ssvep/sunday data/latest/ssvep_8hz.csv')\n",
        "freq15 = pd.read_csv('/content/drive/MyDrive/LeBrain - Neureality 2024/ssvep/sunday data/latest/ssvep_15hz.csv')\n",
        "freq25 = pd.read_csv('/content/drive/MyDrive/LeBrain - Neureality 2024/ssvep/sunday data/latest/ssvep_25hz.csv')\n",
        "freq35 = pd.read_csv('/content/drive/MyDrive/LeBrain - Neureality 2024/ssvep/sunday data/latest/ssvep_35hz.csv')\n",
        "\n",
        "freq8 = freq8.iloc[1200:, 4:8]\n",
        "freq15 = freq15.iloc[1200:, 4:8]\n",
        "freq25 = freq25.iloc[1200:, 4:8]\n",
        "freq35 = freq35.iloc[1200:, 4:8]\n",
        "\n",
        "freq8.columns = ['0', '1', '2', '3']\n",
        "freq15.columns = ['0', '1', '2', '3']\n",
        "freq25.columns = ['0', '1', '2', '3']\n",
        "freq35.columns = ['0', '1', '2', '3']\n",
        "\n",
        "grouped_freq8 = freq8\n",
        "grouped_freq15 = freq15\n",
        "grouped_freq25 = freq25\n",
        "grouped_freq35 = freq35\n",
        "\n",
        "grouped_freq8['label'] = 1\n",
        "grouped_freq15['label'] = 2\n",
        "grouped_freq25['label'] = 3\n",
        "grouped_freq35['label'] = 4\n",
        "\n",
        "grouped_freq8=grouped_freq8[:18125]\n",
        "grouped_freq15=grouped_freq15[:21250]\n",
        "grouped_freq25=grouped_freq25[:21250]\n",
        "grouped_freq35=grouped_freq35[:21250]\n",
        "\n",
        "stacked_df = pd.concat([grouped_freq8, grouped_freq15, grouped_freq25, grouped_freq35], axis=0, ignore_index=True)\n",
        "\n",
        "eeg_channels = stacked_df.iloc[:, 0:4].copy()\n",
        "\n",
        "lowcut = 4.0  # Lower cutoff frequency in Hz\n",
        "highcut = 40.0  # Upper cutoff frequency in Hz\n",
        "sampling_rate = 1000  # Adjust this to your actual sampling rate\n",
        "nyquist = 0.5 * sampling_rate\n",
        "low = lowcut / nyquist\n",
        "high = highcut / nyquist\n",
        "b, a = butter(4, [low, high], btype='band')\n",
        "\n",
        "# Apply the bandpass filter to each column\n",
        "filtered_channels = eeg_channels.apply(lambda column: filtfilt(b, a, column))\n",
        "\n",
        "# Combine the filtered channels with the labels from the original DataFrame\n",
        "filtered_stacked_df = pd.concat([filtered_channels, stacked_df['label']], axis=1)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(filtered_stacked_df)\n",
        "\n",
        "sequence_length = 625\n",
        "num_features = 4\n",
        "\n",
        "appended_arrays = []\n",
        "\n",
        "df_attributes = filtered_stacked_df.drop(columns=['label'])\n",
        "\n",
        "for i in range(0, len(df_attributes) - sequence_length + 1, sequence_length):\n",
        "    sequence_data = df_attributes.iloc[i:i + sequence_length].values\n",
        "\n",
        "    # Reshape the sequence_data to match the input shape for Conv1D\n",
        "    # Calculate real FFT of the sequence\n",
        "    sequence_fft = np.abs(np.fft.fft(sequence_data))\n",
        "\n",
        "    # Calculate average power, total power, and average frequency\n",
        "    average_power = np.mean(sequence_fft**2, axis = 0).reshape(1, -1)\n",
        "    total_power = np.sum(sequence_fft**2, axis = 0).reshape(1, -1)\n",
        "    frequencies = np.fft.fftfreq(len(sequence_data[0]))\n",
        "\n",
        "    # Weighted average frequency using dot product along the correct axis\n",
        "    weighted_avg_frequency = np.sum(frequencies * sequence_fft**2, axis=0).reshape(1, -1) / np.sum(sequence_fft**2, axis=0).reshape(1, -1)\n",
        "\n",
        "\n",
        "    # print(sequence_fft.shape)\n",
        "    # print(sequence_data.shape)\n",
        "    # print(average_power.shape)\n",
        "    # print(total_power.shape)\n",
        "    # print(weighted_avg_frequency.shape)\n",
        "\n",
        "    # Append sequence, FFT, average power, total power, and weighted average frequency to X\n",
        "    new_array = np.concatenate([sequence_data, sequence_fft, average_power, total_power, weighted_avg_frequency], axis=0)\n",
        "    # print(new_array.shape)\n",
        "\n",
        "    appended_arrays.append(new_array)\n",
        "\n",
        "X = np.array(appended_arrays)\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "X = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "#y = np.array(y)\n",
        "# Split data into training and testing sets\n",
        "\n",
        "y = [1] * 29 + [2] * 34 + [3] * 34 + [4] * 34\n",
        "y = np.array(y)\n",
        "one_hot_y = tf.one_hot(y, depth=4).numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, one_hot_y, test_size=0.3, random_state=42)\n",
        "\n",
        "# # Create and apply Conv1D layers\n",
        "# model = Sequential([\n",
        "#     Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(sequence_length*2+3, num_features, 1)),\n",
        "#     MaxPooling1D(pool_size=2),\n",
        "#     Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "#     MaxPooling1D(pool_size=2),\n",
        "#     Flatten(),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(4, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# # Train the model\n",
        "# # model.fit(sequence_data, y, epochs=30, batch_size=2, validation_split=0.2)\n",
        "# # Train the model\n",
        "# model.fit(X_train, y_train, epochs=30, batch_size=2, validation_data=(X_test, y_test))\n",
        "\n",
        "# # Evaluate the model\n",
        "# loss, accuracy = model.evaluate(X_test, y_test)\n",
        "# print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSxxkNHtB-An",
        "outputId": "fa059c77-6e01-4b85-85e6-433a98913f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              0         1         2         3  label\n",
            "0      0.586344  0.305874  1.148232  0.527211      1\n",
            "1      1.461505  0.873929  1.930340  1.098227      1\n",
            "2      2.320186  1.439350  2.699922  1.657165      1\n",
            "3      3.143926  1.991526  3.440917  2.192087      1\n",
            "4      3.915966  2.520977  4.139307  2.692513      1\n",
            "...         ...       ...       ...       ...    ...\n",
            "81870  7.324081  7.103811  7.794086  5.006403      4\n",
            "81871  6.493461  6.139875  6.619907  4.444977      4\n",
            "81872  5.453382  4.977609  5.252977  3.746530      4\n",
            "81873  4.259528  3.674369  3.755854  2.947235      4\n",
            "81874  2.973907  2.293866  2.196679  2.087799      4\n",
            "\n",
            "[81875 rows x 5 columns]\n",
            "(131, 1253, 4)\n",
            "(131, 1253, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout, concatenate, GlobalAveragePooling1D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "input_layer = Input(shape=(sequence_length*2+3, num_features))\n",
        "\n",
        "# 1st Conv1D layer with MaxPooling and Dropout\n",
        "conv1d_1 = Conv1D(filters=64, kernel_size=6, activation='relu', kernel_regularizer=l2(0.1))(input_layer)\n",
        "maxpooling_1 = MaxPooling1D(pool_size=2)(conv1d_1)\n",
        "dropout_1 = Dropout(0.2)(maxpooling_1)\n",
        "\n",
        "# 2nd Conv1D layer with MaxPooling and Dropout\n",
        "conv1d_2 = Conv1D(filters=32, kernel_size=6, activation='relu', kernel_regularizer=l2(0.1))(dropout_1)\n",
        "maxpooling_2 = MaxPooling1D(pool_size=2)(conv1d_2)\n",
        "dropout_2 = Dropout(0.2)(maxpooling_2)\n",
        "\n",
        "# # 3rd Conv1D layer with MaxPooling and Dropout\n",
        "# conv1d_3 = Conv1D(filters=256, kernel_size=6, activation='relu', kernel_regularizer=l2(0.01))(dropout_2)\n",
        "# maxpooling_3 = MaxPooling1D(pool_size=2)(conv1d_3)\n",
        "# dropout_3 = Dropout(0.2)(maxpooling_3)\n",
        "\n",
        "# Flatten layer\n",
        "flatten_layer = Flatten()(dropout_2)\n",
        "\n",
        "# 1st Dense layer\n",
        "dense_1 = Dense(16, activation='relu', kernel_regularizer=l2(0.01))(flatten_layer)\n",
        "dropout_dense_1 = Dropout(0.2)(dense_1)\n",
        "\n",
        "# # 2nd Dense layer\n",
        "# dense_2 = Dense(8, activation='relu', kernel_regularizer=l2(0.01))(dropout_dense_1)\n",
        "# dropout_dense_2 = Dropout(0.2)(dense_2)\n",
        "\n",
        "# Output Dense layer\n",
        "output_layer = Dense(4, activation='softmax')(dropout_dense_1)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='kullback_leibler_divergence', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=4, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {np.mean(loss)}, Test Accuracy: {np.mean(accuracy)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGNXzNrpCNju",
        "outputId": "c0fec653-02d4-46e1-eb1f-b6888e404600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "23/23 [==============================] - 5s 16ms/step - loss: 7.2450 - accuracy: 0.2198 - val_loss: 5.8483 - val_accuracy: 0.2250\n",
            "Epoch 2/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2665 - accuracy: 0.3516 - val_loss: 5.1895 - val_accuracy: 0.2250\n",
            "Epoch 3/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5580 - accuracy: 0.3297 - val_loss: 4.5160 - val_accuracy: 0.3250\n",
            "Epoch 4/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9120 - accuracy: 0.4396 - val_loss: 4.0941 - val_accuracy: 0.3250\n",
            "Epoch 5/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3375 - accuracy: 0.5275 - val_loss: 3.6163 - val_accuracy: 0.3250\n",
            "Epoch 6/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9398 - accuracy: 0.5055 - val_loss: 3.2609 - val_accuracy: 0.3750\n",
            "Epoch 7/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4914 - accuracy: 0.6044 - val_loss: 3.1571 - val_accuracy: 0.2000\n",
            "Epoch 8/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2621 - accuracy: 0.5165 - val_loss: 2.8224 - val_accuracy: 0.3500\n",
            "Epoch 9/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9292 - accuracy: 0.6154 - val_loss: 2.7246 - val_accuracy: 0.3750\n",
            "Epoch 10/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7395 - accuracy: 0.5824 - val_loss: 2.8971 - val_accuracy: 0.3250\n",
            "Epoch 11/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5847 - accuracy: 0.6154 - val_loss: 2.6296 - val_accuracy: 0.2500\n",
            "Epoch 12/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4654 - accuracy: 0.5824 - val_loss: 2.3915 - val_accuracy: 0.2750\n",
            "Epoch 13/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3702 - accuracy: 0.5714 - val_loss: 2.2427 - val_accuracy: 0.2750\n",
            "Epoch 14/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1818 - accuracy: 0.6044 - val_loss: 2.2595 - val_accuracy: 0.2000\n",
            "Epoch 15/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1392 - accuracy: 0.5824 - val_loss: 2.2391 - val_accuracy: 0.2500\n",
            "Epoch 16/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0674 - accuracy: 0.5714 - val_loss: 2.0845 - val_accuracy: 0.2500\n",
            "Epoch 17/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8922 - accuracy: 0.6154 - val_loss: 2.1582 - val_accuracy: 0.2500\n",
            "Epoch 18/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.9729 - accuracy: 0.5165 - val_loss: 1.9904 - val_accuracy: 0.2750\n",
            "Epoch 19/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8070 - accuracy: 0.6044 - val_loss: 2.4192 - val_accuracy: 0.1750\n",
            "Epoch 20/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.8310 - accuracy: 0.5495 - val_loss: 2.0309 - val_accuracy: 0.2250\n",
            "Epoch 21/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.7318 - accuracy: 0.5934 - val_loss: 1.8847 - val_accuracy: 0.3000\n",
            "Epoch 22/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.6154 - val_loss: 2.2775 - val_accuracy: 0.2250\n",
            "Epoch 23/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6840 - accuracy: 0.5934 - val_loss: 1.9460 - val_accuracy: 0.3000\n",
            "Epoch 24/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6347 - accuracy: 0.6154 - val_loss: 1.7439 - val_accuracy: 0.3250\n",
            "Epoch 25/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.6394 - accuracy: 0.6374 - val_loss: 1.6698 - val_accuracy: 0.2750\n",
            "Epoch 26/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6430 - accuracy: 0.5934 - val_loss: 2.0378 - val_accuracy: 0.2250\n",
            "Epoch 27/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5901 - accuracy: 0.5824 - val_loss: 1.6632 - val_accuracy: 0.2750\n",
            "Epoch 28/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.6033 - accuracy: 0.5824 - val_loss: 2.0166 - val_accuracy: 0.2500\n",
            "Epoch 29/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5671 - accuracy: 0.5714 - val_loss: 1.7984 - val_accuracy: 0.2500\n",
            "Epoch 30/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5443 - accuracy: 0.5934 - val_loss: 2.0128 - val_accuracy: 0.1750\n",
            "Epoch 31/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4807 - accuracy: 0.6044 - val_loss: 2.0284 - val_accuracy: 0.2250\n",
            "Epoch 32/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4352 - accuracy: 0.6264 - val_loss: 2.1012 - val_accuracy: 0.1750\n",
            "Epoch 33/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4887 - accuracy: 0.6154 - val_loss: 2.0868 - val_accuracy: 0.1750\n",
            "Epoch 34/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4578 - accuracy: 0.6154 - val_loss: 2.0918 - val_accuracy: 0.1750\n",
            "Epoch 35/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5320 - accuracy: 0.5604 - val_loss: 1.8141 - val_accuracy: 0.2000\n",
            "Epoch 36/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4495 - accuracy: 0.6264 - val_loss: 1.9728 - val_accuracy: 0.1750\n",
            "Epoch 37/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3995 - accuracy: 0.6264 - val_loss: 2.2534 - val_accuracy: 0.2000\n",
            "Epoch 38/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4907 - accuracy: 0.6154 - val_loss: 1.8118 - val_accuracy: 0.2000\n",
            "Epoch 39/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.3308 - accuracy: 0.6484 - val_loss: 2.3206 - val_accuracy: 0.2000\n",
            "Epoch 40/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.4013 - accuracy: 0.6154 - val_loss: 1.9757 - val_accuracy: 0.2500\n",
            "Epoch 41/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.3999 - accuracy: 0.5714 - val_loss: 1.8643 - val_accuracy: 0.2750\n",
            "Epoch 42/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3968 - accuracy: 0.6044 - val_loss: 2.1630 - val_accuracy: 0.2250\n",
            "Epoch 43/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3248 - accuracy: 0.6484 - val_loss: 2.2598 - val_accuracy: 0.1750\n",
            "Epoch 44/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.3675 - accuracy: 0.5934 - val_loss: 2.0274 - val_accuracy: 0.2000\n",
            "Epoch 45/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.2632 - accuracy: 0.6703 - val_loss: 2.4791 - val_accuracy: 0.2000\n",
            "Epoch 46/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2926 - accuracy: 0.6374 - val_loss: 2.5946 - val_accuracy: 0.2000\n",
            "Epoch 47/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3134 - accuracy: 0.6374 - val_loss: 1.9991 - val_accuracy: 0.2500\n",
            "Epoch 48/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3674 - accuracy: 0.6154 - val_loss: 2.0082 - val_accuracy: 0.1500\n",
            "Epoch 49/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3972 - accuracy: 0.6154 - val_loss: 1.7651 - val_accuracy: 0.2250\n",
            "Epoch 50/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.3640 - accuracy: 0.6264 - val_loss: 2.5554 - val_accuracy: 0.1500\n",
            "Epoch 51/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2774 - accuracy: 0.6593 - val_loss: 2.5208 - val_accuracy: 0.2250\n",
            "Epoch 52/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3839 - accuracy: 0.5824 - val_loss: 1.8192 - val_accuracy: 0.2500\n",
            "Epoch 53/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2756 - accuracy: 0.6484 - val_loss: 2.0478 - val_accuracy: 0.2250\n",
            "Epoch 54/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2794 - accuracy: 0.6593 - val_loss: 2.2983 - val_accuracy: 0.1750\n",
            "Epoch 55/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2842 - accuracy: 0.6374 - val_loss: 1.7409 - val_accuracy: 0.2500\n",
            "Epoch 56/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.3028 - accuracy: 0.6484 - val_loss: 2.4353 - val_accuracy: 0.2000\n",
            "Epoch 57/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3528 - accuracy: 0.6044 - val_loss: 1.9504 - val_accuracy: 0.2250\n",
            "Epoch 58/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3682 - accuracy: 0.6154 - val_loss: 2.5544 - val_accuracy: 0.2750\n",
            "Epoch 59/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3534 - accuracy: 0.6264 - val_loss: 2.8664 - val_accuracy: 0.2250\n",
            "Epoch 60/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3926 - accuracy: 0.6264 - val_loss: 2.5335 - val_accuracy: 0.1750\n",
            "Epoch 61/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3896 - accuracy: 0.6374 - val_loss: 2.2283 - val_accuracy: 0.2750\n",
            "Epoch 62/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3816 - accuracy: 0.6593 - val_loss: 2.1829 - val_accuracy: 0.2250\n",
            "Epoch 63/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.2964 - accuracy: 0.6703 - val_loss: 2.0390 - val_accuracy: 0.1750\n",
            "Epoch 64/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3245 - accuracy: 0.6593 - val_loss: 2.7273 - val_accuracy: 0.2250\n",
            "Epoch 65/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.2914 - accuracy: 0.6374 - val_loss: 1.5803 - val_accuracy: 0.2250\n",
            "Epoch 66/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.3485 - accuracy: 0.6044 - val_loss: 1.8841 - val_accuracy: 0.2000\n",
            "Epoch 67/100\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.3487 - accuracy: 0.6484 - val_loss: 2.0525 - val_accuracy: 0.2250\n",
            "Epoch 68/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.3645 - accuracy: 0.6044 - val_loss: 1.7484 - val_accuracy: 0.2500\n",
            "Epoch 69/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2688 - accuracy: 0.6813 - val_loss: 2.5715 - val_accuracy: 0.2000\n",
            "Epoch 70/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.3876 - accuracy: 0.5824 - val_loss: 1.9764 - val_accuracy: 0.2750\n",
            "Epoch 71/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2761 - accuracy: 0.6374 - val_loss: 2.1508 - val_accuracy: 0.2500\n",
            "Epoch 72/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2814 - accuracy: 0.6593 - val_loss: 2.1607 - val_accuracy: 0.2250\n",
            "Epoch 73/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.2735 - accuracy: 0.6264 - val_loss: 2.5928 - val_accuracy: 0.2750\n",
            "Epoch 74/100\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.3831 - accuracy: 0.6044 - val_loss: 1.6053 - val_accuracy: 0.2500\n",
            "Epoch 75/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2354 - accuracy: 0.6593 - val_loss: 2.2791 - val_accuracy: 0.2500\n",
            "Epoch 76/100\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2592 - accuracy: 0.6264 - val_loss: 2.1083 - val_accuracy: 0.3000\n",
            "Epoch 77/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.2756 - accuracy: 0.6484 - val_loss: 2.2568 - val_accuracy: 0.2750\n",
            "Epoch 78/100\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.3316 - accuracy: 0.6374 - val_loss: 2.1893 - val_accuracy: 0.2250\n",
            "Epoch 79/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2444 - accuracy: 0.6484 - val_loss: 2.3743 - val_accuracy: 0.2250\n",
            "Epoch 80/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2295 - accuracy: 0.6703 - val_loss: 2.5046 - val_accuracy: 0.2000\n",
            "Epoch 81/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.2631 - accuracy: 0.6593 - val_loss: 1.7807 - val_accuracy: 0.2750\n",
            "Epoch 82/100\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.2517 - accuracy: 0.6484 - val_loss: 2.1441 - val_accuracy: 0.2750\n",
            "Epoch 83/100\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.2728 - accuracy: 0.6264 - val_loss: 1.9009 - val_accuracy: 0.2250\n",
            "Epoch 84/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2667 - accuracy: 0.6484 - val_loss: 2.2950 - val_accuracy: 0.2250\n",
            "Epoch 85/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2846 - accuracy: 0.6154 - val_loss: 2.0180 - val_accuracy: 0.3000\n",
            "Epoch 86/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2261 - accuracy: 0.6593 - val_loss: 2.3021 - val_accuracy: 0.2500\n",
            "Epoch 87/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3249 - accuracy: 0.6154 - val_loss: 1.6733 - val_accuracy: 0.3250\n",
            "Epoch 88/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.1992 - accuracy: 0.6923 - val_loss: 2.3035 - val_accuracy: 0.2250\n",
            "Epoch 89/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2423 - accuracy: 0.6484 - val_loss: 2.3097 - val_accuracy: 0.2250\n",
            "Epoch 90/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2138 - accuracy: 0.6593 - val_loss: 3.3041 - val_accuracy: 0.2250\n",
            "Epoch 91/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2881 - accuracy: 0.6044 - val_loss: 1.9395 - val_accuracy: 0.2250\n",
            "Epoch 92/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2501 - accuracy: 0.6374 - val_loss: 2.7195 - val_accuracy: 0.2250\n",
            "Epoch 93/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2933 - accuracy: 0.6374 - val_loss: 1.8760 - val_accuracy: 0.2250\n",
            "Epoch 94/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2678 - accuracy: 0.6593 - val_loss: 2.1686 - val_accuracy: 0.2750\n",
            "Epoch 95/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2056 - accuracy: 0.6703 - val_loss: 2.7969 - val_accuracy: 0.2250\n",
            "Epoch 96/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2832 - accuracy: 0.6484 - val_loss: 1.6961 - val_accuracy: 0.2750\n",
            "Epoch 97/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3526 - accuracy: 0.6154 - val_loss: 2.8502 - val_accuracy: 0.2500\n",
            "Epoch 98/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2313 - accuracy: 0.6923 - val_loss: 2.9071 - val_accuracy: 0.2500\n",
            "Epoch 99/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3214 - accuracy: 0.6264 - val_loss: 2.8937 - val_accuracy: 0.2500\n",
            "Epoch 100/100\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2797 - accuracy: 0.6374 - val_loss: 2.1314 - val_accuracy: 0.2250\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.1314 - accuracy: 0.2250\n",
            "Test Loss: 2.1314449310302734, Test Accuracy: 0.22499999403953552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaU4YPXmDyhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}